{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110d24bc-961f-4fc3-bd5a-693bfa67af99",
   "metadata": {},
   "source": [
    "# Cloud Types Segmentation Notebook\n",
    "\n",
    "This notebook allows the user to perform training on a custom UNet with skip connections using SatVision-TOA as the feature extractor.\n",
    "\n",
    "Data for this example is naively created for tutorial purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89237955-6d75-42f1-9b6d-e5bcd1c49272",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installs/imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db5d23-3a38-413e-9b67-7ab1fbc427d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import subprocess\n",
    "import torch.nn as nn\n",
    "import lightning as pl\n",
    "from huggingface_hub import snapshot_download\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e00463-e23c-4de3-a3ec-d0d583731a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"satvision-toa\"\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/nasa-nccs-hpda/satvision-toa\"])\n",
    "else:\n",
    "    subprocess.run([\"git\", \"-C\", repo_dir, \"pull\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a4448-7cea-40b9-800b-9c411c691956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from satvision_toa.utils import load_config\n",
    "from satvision_toa.models.mim import build_mim_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ef0f8-4f7b-43aa-bbe4-7645998692ef",
   "metadata": {},
   "source": [
    "## Define Example Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f255c7c-ab93-41e6-a33e-3d6a7f89dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        num_classes = logits.shape[1]\n",
    "        targets_one_hot = F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float()\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "       \n",
    "        dims = (0, 2, 3)\n",
    "        intersection = torch.sum(probs * targets_one_hot, dims)\n",
    "        cardinality = torch.sum(probs + targets_one_hot, dims)\n",
    "       \n",
    "        dice_score = (2. * intersection + self.smooth) / (cardinality + self.smooth)\n",
    "        return 1. - dice_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62905232-30a7-402a-add1-d1dc04a4ab31",
   "metadata": {},
   "source": [
    "## Define Dummy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd485e8-4d23-4c85-88a9-9bb9c2f01d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyCloudDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, in_channels=3, height=96, width=40, num_classes=9):\n",
    "        self.num_samples = num_samples\n",
    "        self.in_channels = in_channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Random \"ABI-like\" chip (float32, normalized 0â€“1)\n",
    "        chip = torch.rand(self.in_channels, self.height, self.width, dtype=torch.float32)\n",
    "        # Random segmentation mask (integer labels between 0 and num_classes-1)\n",
    "        mask = torch.randint(0, self.num_classes, (self.height, self.width), dtype=torch.long)\n",
    "        return {\"chip\": chip, \"mask\": mask}\n",
    "\n",
    "def get_dummy_dataloader(batch_size=8, num_workers=0):\n",
    "    dataset = DummyCloudDataset()\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3af4d6-e0ee-4a57-82c9-4868df212a27",
   "metadata": {},
   "source": [
    "## Define SatVision-TOA UNet with Skip Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771093d-b1b5-4992-bfa2-dffd609d75e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatVisionUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=9, freeze_encoder=True, output_shape=(96, 40)):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load SatVision encoder\n",
    "        config = load_config()\n",
    "        backbone = build_mim_model(config)\n",
    "        self.encoder = backbone.encoder\n",
    "\n",
    "        # Load pretrained weights\n",
    "        checkpoint = torch.load(config.MODEL.RESUME, weights_only=False)\n",
    "        checkpoint = checkpoint['module']\n",
    "        checkpoint = {k.replace('model.', ''): v for k, v in checkpoint.items() if k.startswith('model')}\n",
    "        self.encoder.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "        # Freeze if requested\n",
    "        if freeze_encoder:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Decoder (same as your U-Net upsampling part)\n",
    "        self.up_trans = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "        ])\n",
    "        self.double_conv_ups = nn.ModuleList([\n",
    "            self._double_conv(1024, 512),\n",
    "            self._double_conv(512, 256),\n",
    "            self._double_conv(256, 128),\n",
    "            self._double_conv(128, 64),\n",
    "        ])\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        self.final_upsample = nn.Upsample(size=output_shape, mode='bilinear', align_corners=False)\n",
    "\n",
    "    def _double_conv(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # SatVision encoder forward\n",
    "\n",
    "        # returns multi-scale features: [stage1, stage2, stage3, stage4]\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # deepest feature\n",
    "        x = features[-1]\n",
    "\n",
    "        # U-Net decoder (skip connections with SatVision features)\n",
    "        skips = features[:-1][::-1]\n",
    "        for up, conv, skip in zip(self.up_trans, self.double_conv_ups, skips):\n",
    "            x = up(x)\n",
    "            if x.shape[2:] != skip.shape[2:]:\n",
    "                x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "            x = torch.cat((skip, x), dim=1)\n",
    "            x = conv(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        x = self.final_upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fef3f0-03bc-4a79-8362-c04cce6688aa",
   "metadata": {},
   "source": [
    "## Define the UNet Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6a188-0d3c-4316-b530-18ccb00f8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatVisionUNetLightning(pl.LightningModule):\n",
    "    def __init__(self, num_classes=9, lr=1e-4, dice_weight=0.5, freeze_encoder=True, target_height=96, target_width=40):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = SatVisionUNet(num_classes=num_classes, freeze_encoder=freeze_encoder, output_shape=(target_height, target_width))\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _common_step(self, batch, stage):\n",
    "        chips, masks = batch[\"chip\"], batch[\"mask\"]\n",
    "        logits = self.forward(chips)\n",
    "        ce = self.ce_loss(logits, masks)\n",
    "        dice = self.dice_loss(logits, masks.long())\n",
    "        loss = self.dice_weight * dice + (1 - self.dice_weight) * ce\n",
    "        self.log(f'{stage}_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, b, i): return self._common_step(b, 'train')\n",
    "    def validation_step(self, b, i): return self._common_step(b, 'val')\n",
    "    def test_step(self, b, i): return self._common_step(b, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0a0c7-fb87-405b-8dae-481f36302b6d",
   "metadata": {},
   "source": [
    "## Define the Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931ee72-9d54-44dd-98e4-e624443be8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SatVisionUNetLightning(\n",
    "    num_classes=9,\n",
    "    lr=0.00001,\n",
    "    dice_weight=0.5,\n",
    "    freeze_encoder=True  # set False if you want to fine-tune SatVision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada095f-04fa-47e5-91b5-d28da0a271c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy DataLoader\n",
    "train_loader = get_dummy_dataloader()\n",
    "val_loader = get_dummy_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c964b-a874-43d4-a960-aa05a9610e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "# Train with dummy data\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ILAB Kernel (Pytorch)",
   "language": "python",
   "name": "pytorch-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
